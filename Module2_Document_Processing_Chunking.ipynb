{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Document Processing & Chunking\n",
    "  \n",
    "**Level:** Beginner to Intermediate  \n",
    "**Prerequisites:** Module 1 completed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "- Explain why chunking is necessary in RAG systems\n",
    "- Implement basic chunking strategies from scratch\n",
    "- Choose appropriate chunk sizes for different use cases\n",
    "- Understand the trade-offs between different chunking approaches\n",
    "- Preserve and use document metadata effectively\n",
    "- Handle different document formats (text, PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Why Chunking Matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The Problem with Whole Documents\n",
    "\n",
    "Imagine you have a 50-page legal contract and someone asks: *\"What is the termination notice period?\"*\n",
    "\n",
    "**Problems with using the whole document:**\n",
    "\n",
    "1. **Too long for context windows**\n",
    "   - Most LLMs have limited context (4K-128K tokens)\n",
    "   - A 50-page document might be 20,000+ tokens\n",
    "   - Can't fit multiple documents in one prompt\n",
    "\n",
    "2. **Poor retrieval precision**\n",
    "   - Embedding a whole document loses specificity\n",
    "   - Can't pinpoint exact relevant section\n",
    "   - Answer might be buried in irrelevant content\n",
    "\n",
    "3. **Inefficient and costly**\n",
    "   - Processing entire documents is slow\n",
    "   - Expensive in terms of tokens/API costs\n",
    "   - Wastes context window space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 How Chunking Solves This\n",
    "\n",
    "**Chunking = Breaking documents into smaller, meaningful pieces**\n",
    "\n",
    "**Benefits:**\n",
    "- Each chunk fits in context window\n",
    "- More precise retrieval (find exact relevant section)\n",
    "- Better embeddings (more specific semantic meaning)\n",
    "- Faster and cheaper processing\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "50-page contract\n",
    "      ↓\n",
    "Split into 100 chunks (500 words each)\n",
    "      ↓\n",
    "Embed each chunk separately\n",
    "      ↓\n",
    "Retrieve only the 3 most relevant chunks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Understanding Chunk Size\n",
    "\n",
    "The most important decision in chunking: **How big should chunks be?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The Chunk Size Trade-off\n",
    "\n",
    "### Small Chunks (100-300 tokens)\n",
    "\n",
    "**Pros:**\n",
    "- Very precise retrieval\n",
    "- Specific answers to specific questions\n",
    "- Less noise in retrieved context\n",
    "\n",
    "**Cons:**\n",
    "- May lose important surrounding context\n",
    "- Might split related information\n",
    "- Need to retrieve more chunks to get full picture\n",
    "\n",
    "**Best for:** FAQ systems, specific fact lookup\n",
    "\n",
    "---\n",
    "\n",
    "### Medium Chunks (300-600 tokens)\n",
    "\n",
    "**Pros:**\n",
    "- Good balance of precision and context\n",
    "- Usually keeps related information together\n",
    "- Most versatile option\n",
    "\n",
    "**Cons:**\n",
    "- Middle ground = not optimized for any specific case\n",
    "\n",
    "**Best for:** Most general RAG use cases (recommended starting point)\n",
    "\n",
    "---\n",
    "\n",
    "### Large Chunks (600-1000+ tokens)\n",
    "\n",
    "**Pros:**\n",
    "- Preserves more context\n",
    "- Better for complex, interconnected information\n",
    "- Fewer chunks to manage\n",
    "\n",
    "**Cons:**\n",
    "- Less precise retrieval\n",
    "- More irrelevant information included\n",
    "- May exceed context limits with multiple chunks\n",
    "\n",
    "**Best for:** research papers, narrative documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Rule of Thumb\n",
    "\n",
    "**Start with 400-500 tokens (roughly 300-400 words)**\n",
    "\n",
    "This is a good default for most use cases. You can adjust based on:\n",
    "- Document type (technical docs → smaller, narratives → larger)\n",
    "- Query types (specific facts → smaller, summaries → larger)\n",
    "- Testing and evaluation results\n",
    "\n",
    "**Remember:** There's no perfect size - it depends on your use case!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "publica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
